{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "48d96168",
      "metadata": {
        "id": "48d96168"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5f665f",
      "metadata": {
        "id": "6b5f665f"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eb67934",
      "metadata": {
        "id": "4eb67934"
      },
      "source": [
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdaf1b80",
      "metadata": {
        "id": "bdaf1b80"
      },
      "source": [
        "# Apache Spark MLIB\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67fa6111",
      "metadata": {
        "id": "67fa6111"
      },
      "source": [
        "Machine Learning Library (MLlib) Guide\n",
        "\n",
        "MLlib is Spark’s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as:\n",
        "\n",
        "    ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
        "    Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
        "    Pipelines: tools for constructing, evaluating, and tuning ML Pipelines\n",
        "    Persistence: saving and load algorithms, models, and Pipelines\n",
        "    Utilities: linear algebra, statistics, data handling, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "721877fb",
      "metadata": {
        "id": "721877fb"
      },
      "source": [
        "Basic Statistics\n",
        "\n",
        "    Correlation\n",
        "    Hypothesis testing\n",
        "        ChiSquareTest\n",
        "    Summarizer\n",
        "\n",
        "Correlation\n",
        "\n",
        "Calculating the correlation between two series of data is a common operation in Statistics. In spark.ml we provide the flexibility to calculate pairwise correlations among many series. The supported correlation methods are currently Pearson’s and Spearman’s correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a33d4f0d",
      "metadata": {
        "id": "a33d4f0d"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create or get existing SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CorrelationExample\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7b27bb43",
      "metadata": {
        "id": "7b27bb43",
        "outputId": "625425f5-e77f-479e-f3cd-fed613933742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(features=SparseVector(4, {0: 1.0, 3: -2.0})),\n",
              " Row(features=DenseVector([4.0, 5.0, 0.0, 3.0])),\n",
              " Row(features=DenseVector([6.0, 7.0, 0.0, 8.0])),\n",
              " Row(features=SparseVector(4, {0: 9.0, 3: 1.0}))]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
        "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
        "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
        "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
        "df = spark.createDataFrame(data, [\"features\"])\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf34dbc4",
      "metadata": {
        "id": "cf34dbc4"
      },
      "source": [
        "# Dense Vector\n",
        "\n",
        "Stores all values explicitly, even if many of them are zero.\n",
        "\n",
        "Backed by a simple array of floats/doubles.\n",
        "# Sparse Vector\n",
        "\n",
        "Stores only the non-zero values and their indices, which saves memory when most entries are zero.\n",
        "\n",
        "Internally represented as (size, indices, values).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "eca5ce95",
      "metadata": {
        "id": "eca5ce95",
        "outputId": "58981498-1130-41ac-8157-36302c640788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation matrix:\n",
            "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
            "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
            "Spearman correlation matrix:\n",
            "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
            "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
          ]
        }
      ],
      "source": [
        "r1 = Correlation.corr(df, \"features\").head()\n",
        "\n",
        "\n",
        "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
        "\n",
        "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
        "\n",
        "\n",
        "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13fa65fb",
      "metadata": {
        "id": "13fa65fb"
      },
      "source": [
        "# Question:   Explain difference between Pearson and Spearman"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pearson correlation** measures the linear relationship between two datasets. It assesses how well the observed data fits a linear equation. The correlation coefficient ranges from -1 to +1, where\n",
        "* +1 indicates a perfect positive linear relationship (as one variable increases, the other increases proportionally).\n",
        "* -1 indicates a perfect negative linear relationship (as one variable increases, the other decreases proportionally).\n",
        "* 0 indicates no linear relationship.\n",
        "\n",
        "**Spearman correlation**, on the other hand, assesses the monotonic relationship between two datasets. It evaluates how well the relationship between two variables can be described using a monotonic function (a function that is either entirely non-increasing or entirely non-decreasing). Instead of using the actual data values, Spearman correlation uses the rank of the data values. This makes it less sensitive to outliers and non-linear relationships compared to Pearson correlation. The Spearman correlation coefficient also ranges from -1 to +1, with similar interpretations regarding the strength and direction of the monotonic relationship.\n",
        "\n",
        "In summary:\n",
        "\n",
        "* Pearson measures linear relationships.\n",
        "* Spearman measures monotonic relationships (based on ranks)."
      ],
      "metadata": {
        "id": "lJvuA4e17tNI"
      },
      "id": "lJvuA4e17tNI"
    },
    {
      "cell_type": "markdown",
      "id": "75dec591",
      "metadata": {
        "id": "75dec591"
      },
      "source": [
        "# Hypothesis testing\n",
        "\n",
        "Hypothesis testing is a powerful tool in statistics to determine whether a result is statistically significant, whether this result occurred by chance or not. spark.ml currently supports Pearson’s Chi-squared ( $\\chi^2$) tests for independence.\n",
        "## ChiSquareTest\n",
        "\n",
        "ChiSquareTest conducts Pearson’s independence test for every feature against the label. For each feature, the (feature, label) pairs are converted into a contingency matrix for which the Chi-squared statistic is computed. All label and feature values must be categorical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "baaf76b3",
      "metadata": {
        "id": "baaf76b3",
        "outputId": "d02fa886-7bd0-4708-a1d4-5fed9b5b0a3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pValues: [0.6872892787909721,0.6822703303362126]\n",
            "degreesOfFreedom: [2, 3]\n",
            "statistics: [0.75,1.5]\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.stat import ChiSquareTest\n",
        "\n",
        "\n",
        "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
        "        (0.0, Vectors.dense(1.5, 20.0)),\n",
        "        (1.0, Vectors.dense(1.5, 30.0)),\n",
        "        (0.0, Vectors.dense(3.5, 30.0)),\n",
        "        (0.0, Vectors.dense(3.5, 40.0)),\n",
        "        (1.0, Vectors.dense(3.5, 40.0))]\n",
        "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
        "\n",
        "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
        "\n",
        "\n",
        "\n",
        "print(\"pValues: \" + str(r.pValues))\n",
        "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
        "print(\"statistics: \" + str(r.statistics))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9af45a20",
      "metadata": {
        "id": "9af45a20"
      },
      "source": [
        "If two categorical variables A and B are independent:\n",
        "\n",
        "* P(A=i,B=j)=P(A=i) P(B=j)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08dbb0eb",
      "metadata": {
        "id": "08dbb0eb"
      },
      "source": [
        "N = 80\n",
        "$$\n",
        "P(\\text{Male}) = \\tfrac{40}{80} = 0.5, \\quad\n",
        "P(\\text{Female}) = \\tfrac{40}{80} = 0.5\n",
        "$$\n",
        "$$\n",
        "P(\\text{Likes}) = \\tfrac{50}{80} = 0.625, \\quad\n",
        "P(\\text{Doesn't}) = \\tfrac{30}{80} = 0.375\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fe2072e",
      "metadata": {
        "id": "0fe2072e"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd90c2d",
      "metadata": {
        "id": "bbd90c2d"
      },
      "source": [
        "$$\n",
        "E(\\text{Male, Likes})    = 80 \\cdot 0.5 \\cdot 0.625 = 25\n",
        "$$\n",
        "$$\n",
        "E(\\text{Male, Doesn't})  = 80 \\cdot 0.5 \\cdot 0.375 = 15\n",
        "$$\n",
        "$$\n",
        "E(\\text{Female, Likes})  = 80 \\cdot 0.5 \\cdot 0.625 = 25\n",
        "$$\n",
        "$$\n",
        "E(\\text{Female, Doesn't})  = 80 \\cdot 0.5 \\cdot 0.375 = 25\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42025cc2",
      "metadata": {
        "id": "42025cc2"
      },
      "source": [
        "### Chi-Square Test Calculation\n",
        "\n",
        "We compare observed $(O)$ and expected $(E)$ frequencies using:\n",
        "\n",
        "$$\n",
        "\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b373dca1",
      "metadata": {
        "id": "b373dca1"
      },
      "source": [
        "We compare observedvalue with Expected value\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{(30-25)^2}{25} &= 1.0 \\\\\n",
        "\\frac{(10-15)^2}{15} &= 1.67 \\\\\n",
        "\\frac{(20-25)^2}{25} &= 1.0 \\\\\n",
        "\\frac{(20-15)^2}{15} &= 1.67\n",
        "\\end{align*}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ec471b",
      "metadata": {
        "id": "03ec471b"
      },
      "source": [
        "$$\n",
        "\\chi^2 = 1.0 + 1.67 + 1.0 + 1.67 \\approx 5.33\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a561ebbf",
      "metadata": {
        "id": "a561ebbf"
      },
      "source": [
        "χ² = 0 → complete independence (perfect match).\n",
        "\n",
        "χ² small → close to independence (differences are just noise).\n",
        "\n",
        "\n",
        "### χ² large → strong evidence they are not independent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "220223ec",
      "metadata": {
        "id": "220223ec"
      },
      "source": [
        "In ML feature selection (Spark, scikit-learn, etc.)\n",
        "\n",
        "You run χ² between each categorical feature and the target (label).\n",
        "\n",
        "* Features with low p-values are likely informative.\n",
        "\n",
        "* Features with high p-values look like noise and can be dropped."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ca51bdd",
      "metadata": {
        "id": "0ca51bdd"
      },
      "source": [
        "# Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "477b5174",
      "metadata": {
        "id": "477b5174",
        "outputId": "454c30ef-ffe9-408f-b397-f98caf51f7e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+\n",
            "|aggregate_metrics(features, weight)|\n",
            "+-----------------------------------+\n",
            "|{[1.0,1.0,1.0], 1, [0.0,0.0,0.0]}  |\n",
            "+-----------------------------------+\n",
            "\n",
            "+---------------------------------------------------------------+\n",
            "|aggregate_metrics(features, 1.0)                               |\n",
            "+---------------------------------------------------------------+\n",
            "|{[1.0,1.5,2.0], 2, [0.0,0.7071067811865476,1.4142135623730951]}|\n",
            "+---------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.stat import Summarizer\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = spark.createDataFrame([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
        "                            Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))])\n",
        "\n",
        "# create summarizer for multiple metrics \"mean\" and \"count\"\n",
        "summarizer = Summarizer.metrics(\"mean\", \"count\",\"std\")\n",
        "#Available metrics are:\n",
        "#the column-wise max, min, mean, sum, variance, std, and number of nonzeros.\n",
        "# compute statistics for multiple metrics with weight\n",
        "df.select(summarizer.summary(df.features, df.weight, )).show(truncate=False)\n",
        "df.select(summarizer.summary(df.features )).show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "416d5368",
      "metadata": {
        "id": "416d5368"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3c243da",
      "metadata": {
        "id": "a3c243da"
      },
      "source": [
        "# PIPELINES\n",
        "MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow.\n",
        "Mainly inspired in SCIKIT LEARN SO! very similar"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0420d765",
      "metadata": {
        "id": "0420d765"
      },
      "source": [
        "## Main classes :\n",
        "* DataFrames:\n",
        "\n",
        "\n",
        "* Transformer: A Transformer is an algorithm which can transform one DataFrame into another DataFrame.  E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n",
        "\n",
        "* Estimator: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n",
        "\n",
        "* Pipeline: A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
        "\n",
        "Parameter: All Transformers and Estimators now share a common API for specifying parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b0228cd",
      "metadata": {
        "id": "7b0228cd"
      },
      "source": [
        "# Pipeline components\n",
        "## Transformers\n",
        "\n",
        "* A feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.\n",
        "* A learning model might take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and output a new DataFrame with predicted labels appended as a column.\n",
        "\n",
        "## Estimators\n",
        "\n",
        "* Technically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer.\n",
        "* For example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6239053",
      "metadata": {
        "id": "f6239053"
      },
      "source": [
        "# Pipeline\n",
        "In machine learning, it is common to run a sequence of algorithms to process and learn from data. E.g., a simple text document processing workflow might include several stages:\n",
        "\n",
        "    Split each document’s text into words.\n",
        "    Convert each document’s words into a numerical feature vector.\n",
        "    Learn a prediction model using the feature vectors and labels.\n",
        "\n",
        "MLlib represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. We will use this simple workflow as a running example in this section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed1b73c",
      "metadata": {
        "id": "eed1b73c"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9af4f0dc",
      "metadata": {
        "id": "9af4f0dc"
      },
      "source": [
        "The first two (Tokenizer and HashingTF) are Transformers (blue), and the third (LogisticRegression) is an Estimator (red). The bottom row represents data flowing through the pipeline, where cylinders indicate DataFrames. The Pipeline.fit() method is called on the original DataFrame, which has raw text documents and labels. The Tokenizer.transform() method splits the raw text documents into words, adding a new column with words to the DataFrame. The HashingTF.transform() method converts the words column into feature vectors, adding a new column with those vectors to the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e8aac2a9",
      "metadata": {
        "id": "e8aac2a9"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "# Prepare training data from a list of (label, features) tuples.\n",
        "training = spark.createDataFrame([\n",
        "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
        "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
        "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
        "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
        "\n",
        "# Create a LogisticRegression instance. This instance is an Estimator.\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
        "# Print out the parameters, documentation, and any default values.\n",
        "#print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
        "\n",
        "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
        "model1 = lr.fit(training)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04394c73",
      "metadata": {
        "id": "04394c73",
        "outputId": "7fcc0da2-d05b-4f1f-dae5-8ef54104b93a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 was fit using parameters: \n",
            "Model 2 was fit using parameters: \n"
          ]
        }
      ],
      "source": [
        "# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n",
        "# we can view the parameters it used during fit().\n",
        "# This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
        "# LogisticRegression instance.\n",
        "print(\"Model 1 was fit using parameters: \")\n",
        "#print(model1.extractParamMap())\n",
        "\n",
        "# We may alternatively specify parameters using a Python dictionary as a paramMap\n",
        "paramMap = {lr.maxIter: 20}\n",
        "paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\n",
        "# Specify multiple Params.\n",
        "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})\n",
        "\n",
        "# You can combine paramMaps, which are python dictionaries.\n",
        "# Change output column name\n",
        "paramMap2 = {lr.probabilityCol: \"myProbability\"}\n",
        "paramMapCombined = paramMap.copy()\n",
        "paramMapCombined.update(paramMap2)  # type: ignore\n",
        "\n",
        "# Now learn a new model using the paramMapCombined parameters.\n",
        "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
        "model2 = lr.fit(training, paramMapCombined)\n",
        "print(\"Model 2 was fit using parameters: \")\n",
        "#print(model2.extractParamMap())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22c132ad",
      "metadata": {
        "id": "22c132ad",
        "outputId": "20bd7e61-f45f-471a-e51f-32efe4024b61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features=[-1.0,1.5,1.3], label=1.0 -> prob=[0.05707304993572537,0.9429269500642746], prediction=1.0\n",
            "features=[3.0,2.0,-0.1], label=0.0 -> prob=[0.9238521956443227,0.07614780435567725], prediction=0.0\n",
            "features=[0.0,2.2,-1.5], label=1.0 -> prob=[0.10972780286187774,0.8902721971381222], prediction=1.0\n"
          ]
        }
      ],
      "source": [
        "# Prepare test data\n",
        "test = spark.createDataFrame([\n",
        "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
        "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
        "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
        "\n",
        "# Make predictions on test data using the Transformer.transform() method.\n",
        "# LogisticRegression.transform will only use the 'features' column.\n",
        "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
        "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
        "prediction = model2.transform(test)\n",
        "result = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\") \\\n",
        "    .collect()\n",
        "\n",
        "for row in result:\n",
        "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
        "          % (row.features, row.label, row.myProbability, row.prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757e66b5",
      "metadata": {
        "id": "757e66b5"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8f49c19",
      "metadata": {
        "id": "a8f49c19"
      },
      "source": [
        "# There are many models available, be sure to choose at least 3 and try to fit them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e69f9e05",
      "metadata": {
        "id": "e69f9e05",
        "outputId": "a85e2bd5-b4c0-4a86-b2a6-6de59b6c3ba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, spark i j k) --> prob=[0.6292098489668484,0.3707901510331516], prediction=0.000000\n",
            "(5, l m n) --> prob=[0.984770006762304,0.015229993237696027], prediction=0.000000\n",
            "(6, spark hadoop spark) --> prob=[0.13412348342566158,0.8658765165743384], prediction=1.000000\n",
            "(7, apache hadoop) --> prob=[0.9955732114398529,0.00442678856014711], prediction=0.000000\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "\n",
        "# Prepare training documents from a list of (id, text, label) tuples.\n",
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 0.0)\n",
        "], [\"id\", \"text\", \"label\"])\n",
        "\n",
        "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "model = pipeline.fit(training)\n",
        "\n",
        "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"spark hadoop spark\"),\n",
        "    (7, \"apache hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "\n",
        "# Make predictions on test documents and print columns of interest.\n",
        "prediction = model.transform(test)\n",
        "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "for row in selected.collect():\n",
        "    rid, text, prob, prediction = row\n",
        "    print(\n",
        "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
        "            rid, text, str(prob), prediction   # type: ignore\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b13339",
      "metadata": {
        "id": "47b13339"
      },
      "source": [
        "## On ROW 4!\n",
        "Even though \"spark\" is there, the model also sees \"i j k\" (never in training), so \"spark\" alone wasn’t enough to push probability > 0.5."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb72e51",
      "metadata": {
        "id": "acb72e51"
      },
      "source": [
        "Details  of tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2e9c91ce",
      "metadata": {
        "id": "2e9c91ce",
        "outputId": "31dfc835-aab5-4ffe-8b6d-fae45ade371e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(20,[6,8,13,16],[...|\n",
            "|  0.0|(20,[0,2,7,13,15,...|\n",
            "|  1.0|(20,[3,4,6,11,19]...|\n",
            "+-----+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0.0, \"Hi I heard about Spark\"),\n",
        "    (0.0, \"I wish Java could use case classes\"),\n",
        "    (1.0, \"Logistic regression models are neat\")\n",
        "], [\"label\", \"sentence\"])\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84404f6",
      "metadata": {
        "id": "a84404f6"
      },
      "source": [
        "# Question: Please explain what a Tokenizer does, and how does it do it, how about the hasher?\n",
        "## hint hash\n",
        "h:{all strings}→{integers}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer**\n",
        "\n",
        "A `Tokenizer` is a feature transformer that breaks down a text document into individual words or \"tokens\". It does this by splitting the text based on whitespace and sometimes punctuation.\n",
        "\n",
        "*   **How it works:** The `Tokenizer` takes a column of text (strings) as input and outputs a new column containing arrays of strings, where each string in the array is a word from the original text. For example, the sentence \"Hi I heard about Spark\" would be tokenized into the array `[\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]`.\n",
        "\n",
        "**HashingTF**\n",
        "\n",
        "`HashingTF` (Hashing Term Frequency) is another feature transformer that converts a collection of text documents (represented as arrays of tokens) into numerical feature vectors. It uses the hashing trick to map terms (words) to indices in a fixed-size vector.\n",
        "\n",
        "*   **How it works:** Instead of building a vocabulary of all unique words, which can be memory-intensive for large datasets, `HashingTF` applies a hash function to each word. The output of the hash function is used as an index in a feature vector. The value at that index is incremented each time the word appears in the document. This process creates a vector where each element represents the frequency of words that hash to that particular index.\n",
        "\n",
        "    The number of features in the output vector is determined by the `numFeatures` parameter. A larger `numFeatures` reduces the likelihood of hash collisions (different words mapping to the same index).\n",
        "\n",
        "In summary:\n",
        "\n",
        "1.  **Tokenizer:** Splits text into words.\n",
        "2.  **HashingTF:** Converts words into numerical feature vectors using a hash function and term frequency."
      ],
      "metadata": {
        "id": "Ck5cecLG8z7j"
      },
      "id": "Ck5cecLG8z7j"
    },
    {
      "cell_type": "markdown",
      "id": "d05b37b5",
      "metadata": {
        "id": "d05b37b5"
      },
      "source": [
        "# Word2Vec\n",
        "\n",
        "Word2Vec is an Estimator which takes sequences of words representing documents and trains a Word2VecModel. The model maps each word to a unique fixed-size vector. The Word2VecModel transforms each document into a vector using the average of all words in the document; this vector can then be used as features for prediction, document similarity calculations, etc. Please refer to the MLlib user guide on Word2Vec for more details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "21233c1c",
      "metadata": {
        "id": "21233c1c",
        "outputId": "42b77b22-4a25-45b5-c49f-bbea61ba0601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: [Hi, I, heard, about, Spark] => \n",
            "Vector: [-0.021449678391218186,0.016573484241962432,0.030733881890773775]\n",
            "\n",
            "Text: [I, wish, Java, could, use, case, classes] => \n",
            "Vector: [-0.05426883537854467,-0.059132851128067286,0.06164071549262319]\n",
            "\n",
            "Text: [Logistic, regression, models, are, neat] => \n",
            "Vector: [0.01564687564969063,0.0912977852858603,-0.056992095476016406]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "\n",
        "# Input data: Each row is a bag of words from a sentence or document.\n",
        "documentDF = spark.createDataFrame([\n",
        "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
        "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
        "    (\"Logistic regression models are neat\".split(\" \"), )\n",
        "], [\"text\"])\n",
        "\n",
        "# Learn a mapping from words to Vectors.\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
        "model = word2Vec.fit(documentDF)\n",
        "\n",
        "result = model.transform(documentDF)\n",
        "for row in result.collect():\n",
        "    text, vector = row\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "598fc912",
      "metadata": {
        "id": "598fc912"
      },
      "source": [
        "# Model selection (a.k.a. hyperparameter tuning)\n",
        "\n",
        "An important task in ML is model selection, or using data to find the best model or parameters for a given task. This is also called tuning. Tuning may be done for individual Estimators such as LogisticRegression, or for entire Pipelines which include multiple algorithms, featurization, and other steps.\n",
        "Users can tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "341fd437",
      "metadata": {
        "id": "341fd437"
      },
      "source": [
        "## MLlib supports model selection using tools such as CrossValidator and TrainValidationSplit.\n",
        "They Require:\n",
        "\n",
        "*    Estimator: algorithm or Pipeline to tune\n",
        "*    Set of ParamMaps: parameters to choose from, sometimes called a “parameter grid” to search over\n",
        "*    Evaluator: metric to measure how well a fitted Model does on held-out test data\n",
        "\n",
        "## At a high level, these model selection tools work as follows:\n",
        "\n",
        "*    They split the input data into separate training and test datasets.\n",
        "*    For each (training, test) pair, they iterate through the set of ParamMaps:\n",
        "*    For each ParamMap, they fit the Estimator using those parameters, get the fitted Model, and evaluate the Model’s performance using the Evaluator.\n",
        "*    They select the Model produced by the best-performing set of parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d16409d",
      "metadata": {
        "id": "4d16409d"
      },
      "source": [
        "# Cross-Validation\n",
        "\n",
        "CrossValidator begins by splitting the dataset into a set of folds which are used as separate training and test datasets. E.g., with $k=3$ folds, CrossValidator will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular ParamMap, CrossValidator computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the 3 different (training, test) dataset pairs.\n",
        "\n",
        "After identifying the best ParamMap, CrossValidator finally re-fits the Estimator using the best ParamMap and the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "394084e6",
      "metadata": {
        "id": "394084e6",
        "outputId": "35004370-917c-4a9c-fe30-975eca58e97e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(id=4, text='spark i j k', probability=DenseVector([0.2665, 0.7335]), prediction=1.0)\n",
            "Row(id=5, text='l m n', probability=DenseVector([0.9204, 0.0796]), prediction=0.0)\n",
            "Row(id=6, text='mapreduce spark', probability=DenseVector([0.4438, 0.5562]), prediction=1.0)\n",
            "Row(id=7, text='apache hadoop', probability=DenseVector([0.8587, 0.1413]), prediction=0.0)\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Prepare training documents, which are labeled.\n",
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 0.0),\n",
        "    (4, \"b spark who\", 1.0),\n",
        "    (5, \"g d a y\", 0.0),\n",
        "    (6, \"spark fly\", 1.0),\n",
        "    (7, \"was mapreduce\", 0.0),\n",
        "    (8, \"e spark program\", 1.0),\n",
        "    (9, \"a e c l\", 0.0),\n",
        "    (10, \"spark compile\", 1.0),\n",
        "    (11, \"hadoop software\", 0.0)\n",
        "], [\"id\", \"text\", \"label\"])\n",
        "\n",
        "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
        "\n",
        "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
        "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
        "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
        "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
        "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
        "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .build()\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=BinaryClassificationEvaluator(),\n",
        "                          numFolds=2)  # use 3+ folds in practice\n",
        "\n",
        "# Run cross-validation, and choose the best set of parameters.\n",
        "cvModel = crossval.fit(training)\n",
        "\n",
        "# Prepare test documents, which are unlabeled.\n",
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"mapreduce spark\"),\n",
        "    (7, \"apache hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "\n",
        "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
        "prediction = cvModel.transform(test)\n",
        "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "for row in selected.collect():\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b37fabc",
      "metadata": {
        "id": "9b37fabc"
      },
      "source": [
        "# Deep Learning\n",
        "Spark is well able to perform deep learning, it has some apllications already as spark local and good supoort for the most commn frameworks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb5866b4",
      "metadata": {
        "id": "cb5866b4"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b8b5f8b5",
      "metadata": {
        "id": "b8b5f8b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f369ec6-ff56-4c4b-af7b-30005e394aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Naive Bayes ===\n",
            "+---+---------------+----------------------------------------+----------+\n",
            "|id |text           |probability                             |prediction|\n",
            "+---+---------------+----------------------------------------+----------+\n",
            "|4  |spark i j k    |[0.12522710941200788,0.8747728905879921]|1.0       |\n",
            "|5  |l m n          |[0.7685811612217593,0.23141883877824074]|0.0       |\n",
            "|6  |mapreduce spark|[0.15136226034308783,0.8486377396569121]|1.0       |\n",
            "|7  |apache hadoop  |[0.8652459541740107,0.13475404582598927]|0.0       |\n",
            "+---+---------------+----------------------------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "\n",
        "nb = NaiveBayes(modelType=\"multinomial\", labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "pipeline_nb = Pipeline(stages=[tokenizer, hashingTF, nb])\n",
        "\n",
        "paramGrid_nb = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
        "    .build()\n",
        "\n",
        "crossval_nb = CrossValidator(estimator=pipeline_nb,\n",
        "                             estimatorParamMaps=paramGrid_nb,\n",
        "                             evaluator=BinaryClassificationEvaluator(),\n",
        "                             numFolds=2)\n",
        "\n",
        "cvModel_nb = crossval_nb.fit(training)\n",
        "\n",
        "prediction_nb = cvModel_nb.transform(test)\n",
        "print(\"=== Naive Bayes ===\")\n",
        "prediction_nb.select(\"id\", \"text\", \"probability\", \"prediction\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
        "\n",
        "pipeline_rf = Pipeline(stages=[tokenizer, hashingTF, rf])\n",
        "\n",
        "paramGrid_rf = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
        "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
        "    .build()\n",
        "\n",
        "crossval_rf = CrossValidator(estimator=pipeline_rf,\n",
        "                             estimatorParamMaps=paramGrid_rf,\n",
        "                             evaluator=BinaryClassificationEvaluator(),\n",
        "                             numFolds=2)\n",
        "\n",
        "cvModel_rf = crossval_rf.fit(training)\n",
        "\n",
        "prediction_rf = cvModel_rf.transform(test)\n",
        "print(\"=== Random Forest ===\")\n",
        "prediction_rf.select(\"id\", \"text\", \"probability\", \"prediction\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uie2k2a_h1S",
        "outputId": "3d1b5458-7997-4819-828d-b05b15e5f5c0"
      },
      "id": "-uie2k2a_h1S",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Random Forest ===\n",
            "+---+---------------+---------------+----------+\n",
            "|id |text           |probability    |prediction|\n",
            "+---+---------------+---------------+----------+\n",
            "|4  |spark i j k    |[0.0125,0.9875]|1.0       |\n",
            "|5  |l m n          |[1.0,0.0]      |0.0       |\n",
            "|6  |mapreduce spark|[0.2125,0.7875]|1.0       |\n",
            "|7  |apache hadoop  |[1.0,0.0]      |0.0       |\n",
            "+---+---------------+---------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
        "\n",
        "pipeline_gbt = Pipeline(stages=[tokenizer, hashingTF, gbt])\n",
        "\n",
        "paramGrid_gbt = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
        "    .addGrid(gbt.maxDepth, [3, 5]) \\\n",
        "    .build()\n",
        "\n",
        "crossval_gbt = CrossValidator(estimator=pipeline_gbt,\n",
        "                              estimatorParamMaps=paramGrid_gbt,\n",
        "                              evaluator=BinaryClassificationEvaluator(),\n",
        "                              numFolds=2)\n",
        "\n",
        "cvModel_gbt = crossval_gbt.fit(training)\n",
        "\n",
        "prediction_gbt = cvModel_gbt.transform(test)\n",
        "print(\"=== Gradient-Boosted Trees ===\")\n",
        "prediction_gbt.select(\"id\", \"text\", \"probability\", \"prediction\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdKIzMXV_m3L",
        "outputId": "509db561-0394-447b-9aa0-0a64e48ccc00"
      },
      "id": "VdKIzMXV_m3L",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Gradient-Boosted Trees ===\n",
            "+---+---------------+-----------------------------------------+----------+\n",
            "|id |text           |probability                              |prediction|\n",
            "+---+---------------+-----------------------------------------+----------+\n",
            "|4  |spark i j k    |[0.04364652142729318,0.9563534785727068] |1.0       |\n",
            "|5  |l m n          |[0.9563534785727067,0.043646521427293306]|0.0       |\n",
            "|6  |mapreduce spark|[0.04364652142729318,0.9563534785727068] |1.0       |\n",
            "|7  |apache hadoop  |[0.9563534785727067,0.043646521427293306]|0.0       |\n",
            "+---+---------------+-----------------------------------------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Mesa ENV",
      "language": "python",
      "name": "mesa-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}